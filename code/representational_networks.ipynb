{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7rsokwk8noHNnbCP/cvka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hallpaz/nov23google/blob/main/code/representational_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representational Networks\n",
        "\n",
        "## Hallison Paz\n",
        "\n",
        "### November 8th, 2023\n",
        "\n",
        "Lecture given at Google Brasil's office at São Paulo.\n",
        "\n",
        "Additional resources available at [this repository](https://github.com/hallpaz/googlesptalk).\n",
        "\n"
      ],
      "metadata": {
        "id": "MOH-dXcmq4pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''<iframe width=\"560\" height=\"315\"\n",
        "        src=\"https://www.youtube.com/embed/_ZtQ0-tDwbY\"\n",
        "        frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media;\n",
        "        gyroscope; picture-in-picture\" allowfullscreen></iframe>''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "vcsTEwW-k-YN",
        "outputId": "68219ba4-80ac-4874-b0ac-ba422d85996a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" \n",
              "        src=\"https://www.youtube.com/embed/_ZtQ0-tDwbY\" \n",
              "        frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; \n",
              "        gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Representational Network for Images"
      ],
      "metadata": {
        "id": "94IM-IYYlwP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bINCN5U9qxns"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import numpy as np\n",
        "from typing import Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a dataset structure"
      ],
      "metadata": {
        "id": "yuZ9-TidmHNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_grid_coords(nsamples, start, end, dim, flatten=True):\n",
        "  if not isinstance(nsamples, Sequence):\n",
        "      nsamples = dim * [nsamples]\n",
        "  if not isinstance(start, Sequence):\n",
        "      start = dim * [start]\n",
        "  if not isinstance(end, Sequence):\n",
        "      end = dim * [end]\n",
        "  if len(nsamples) != dim or len(start) != dim or len(end) != dim:\n",
        "      raise ValueError(\"'nsamples'; 'start'; and 'end' should be a single value or have same  length as 'dim'\")\n",
        "\n",
        "  dir_samples = tuple([torch.linspace(start[i], end[i], steps=nsamples[i])\n",
        "                  for i in range(dim)])\n",
        "  grid = torch.stack(torch.meshgrid(*dir_samples, indexing='ij'), dim=-1)\n",
        "  return grid.reshape(-1, dim) if flatten else grid\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, filepath, size=0, color_space='L'):\n",
        "    super().__init__()\n",
        "    img = Image.open(filepath).convert(color_space)\n",
        "    if size > 0:\n",
        "      img = img.resize((size, size))\n",
        "    else:\n",
        "      size = img.width\n",
        "    if color_space == 'L':\n",
        "      self.channels = 1\n",
        "    else:\n",
        "      self.channels = 3\n",
        "    # N x 2; N = width * height\n",
        "    self.coords = make_grid_coords(size, -1, 1, 2)\n",
        "    # N x (1 ou 3)\n",
        "    self.pixels = to_tensor(img).permute(1, 2, 0).reshape(-1, self.channels)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    #return len(self.pixels)\n",
        "    return 1\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.coords, self.pixels\n",
        "\n",
        "def network_to_image(model, channels=3, res=512,\n",
        "                     return_img=False, device='cpu'):\n",
        "  coords = make_grid_coords(res, -1, 1, 2).to(device)\n",
        "  pixels = model(coords).clamp(0, 1).reshape(res, res, channels)\n",
        "  img = pixels.squeeze(-1).detach().cpu().numpy()\n",
        "  if return_img:\n",
        "    return img\n",
        "  plt.imshow(img)"
      ],
      "metadata": {
        "id": "2L-oI1hirWOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota:** Imagem do Masp é um recorte de: https://www.flickr.com/photos/governosp/52692996751"
      ],
      "metadata": {
        "id": "lNp6plDvXR30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/hallpaz/nov23google/blob/ecca56318491c043b4741cd230073a9d9a76fac1/img/masp.jpg?raw=true -O masp.jpg"
      ],
      "metadata": {
        "id": "zM7vecMNmSfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper = {\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    'channels': 3,\n",
        "    'epochs': 1000\n",
        "}"
      ],
      "metadata": {
        "id": "2ZdaulsTYU87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageDataset(\"/content/masp.jpg\", hyper['width'], \"RGB\")\n",
        "dataloader = DataLoader(dataset, hyper['width'] * hyper['width'])"
      ],
      "metadata": {
        "id": "Pm4rXFvgYDi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, pixels = dataset[0]\n",
        "pixels = pixels.reshape(hyper['width'],\n",
        "               hyper['height'],\n",
        "               hyper['channels'])\n",
        "plt.imshow(pixels)"
      ],
      "metadata": {
        "id": "tl8D8V1TZDgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training routine"
      ],
      "metadata": {
        "id": "mbb89jePmonj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, hyper, device,\n",
        "          steps_til_summary=20, gif_path=\"\"):\n",
        "    dim = hyper['width']\n",
        "    epochs = hyper['epochs']\n",
        "    channels = hyper['channels']\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    optim = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
        "    model_input, ground_truth = next(iter(dataloader))\n",
        "    model_input, ground_truth = model_input.to(device), ground_truth.to(device)\n",
        "\n",
        "    if gif_path:\n",
        "        writer = imageio.get_writer(gif_path, mode='I', duration=0.3)\n",
        "\n",
        "    for step in range(epochs):\n",
        "        model_output = model(model_input.to(device))\n",
        "        loss = ((model_output - ground_truth)**2).mean()\n",
        "\n",
        "        if not (step % steps_til_summary):\n",
        "            print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "            # plot_tensor_img(model_output, dim)\n",
        "            network_to_image(model, channels, dim, device=device)\n",
        "\n",
        "        if gif_path and (step % 5 == 0):\n",
        "            img = network_to_image(model, channels, dim, True, device)\n",
        "            writer.append_data(np.uint8(img * 255))\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    # last inference\n",
        "    model.eval()\n",
        "    network_to_image(model, hyper['channels'], return_img=True, device=device)\n",
        "    if gif_path:\n",
        "      writer.append_data(np.uint8(img * 255))\n",
        "      writer.close()"
      ],
      "metadata": {
        "id": "kQwMh19bW6Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a Neural Network model"
      ],
      "metadata": {
        "id": "Tq9PMFASmU3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReluNetwork(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(2, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.layers(input)\n"
      ],
      "metadata": {
        "id": "pAxkQBSLrZhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu_model = ReluNetwork(3)\n",
        "network_to_image(relu_model, 3, hyper['width'])"
      ],
      "metadata": {
        "id": "RxH4NywPrj_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train(relu_model,\n",
        "      dataloader,\n",
        "      hyper,\n",
        "      device,\n",
        "      gif_path='masp.gif')"
      ],
      "metadata": {
        "id": "r-pD3QNFYT-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FourierNetwork(nn.Module):\n",
        "  def __init__(self, omega_0=30, channels=1):\n",
        "    super().__init__()\n",
        "    self.omega_0 = omega_0\n",
        "    self.first_layer = nn.Linear(2, 256)\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, channels)\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "      self.first_layer.weight.uniform_(-1 / 2, 1 / 2)\n",
        "\n",
        "  def forward(self, coords):\n",
        "    x = torch.sin(self.omega_0 * self.first_layer(coords))\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "BOLrfWKOdLUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fourier_model = FourierNetwork(30, 3)\n",
        "train(fourier_model,\n",
        "      dataloader,\n",
        "      hyper,\n",
        "      device,\n",
        "      gif_path='fourier_masp.gif')"
      ],
      "metadata": {
        "id": "SYIMTegGdMAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "\n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
        "    # hyperparameter.\n",
        "\n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "\n",
        "    def forward_with_intermediate(self, input):\n",
        "        # For visualization of activation distributions\n",
        "        intermediate = self.omega_0 * self.linear(input)\n",
        "        return torch.sin(intermediate), intermediate\n",
        "\n",
        "\n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False,\n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, hidden_features,\n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0,\n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "\n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, coords):\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        # return output, coords\n",
        "        return output"
      ],
      "metadata": {
        "id": "YwmYjiJ7gQRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siren_model = Siren(2, 256, 2, 3, True, 30)\n",
        "train(siren_model,\n",
        "      dataloader,\n",
        "      hyper,\n",
        "      device,\n",
        "      gif_path='siren_masp.gif')"
      ],
      "metadata": {
        "id": "az97o1sVggNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the model's space continuity"
      ],
      "metadata": {
        "id": "oMpO6fVTm597"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, Box, interact_manual"
      ],
      "metadata": {
        "id": "XKdP9-lvwNTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slider = widgets.FloatRangeSlider(\n",
        "    value=[-1.0, 1.0],\n",
        "    min=-7,\n",
        "    max=7,\n",
        "    step=0.1,\n",
        "    description='Interval:',\n",
        "    disabled=False,\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.1f',\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")"
      ],
      "metadata": {
        "id": "9UE8idoICq3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = siren_model\n",
        "res = hyper['width']\n",
        "channels = hyper['channels']\n",
        "def plot_model(interval):\n",
        "  model.to(device)\n",
        "  grid = make_grid_coords(res, *interval, dim=2).to(device)\n",
        "  output = model(grid)\n",
        "  model_out = torch.clamp(output, 0.0, 1.0)\n",
        "\n",
        "  pixels = model_out.cpu().detach().view(res, res, channels).cpu()\n",
        "  pixels = (pixels * 255).numpy().astype(np.uint8)\n",
        "  if channels == 1:\n",
        "      pixels = np.repeat(pixels, 3, axis=-1)\n",
        "  return Image.fromarray(pixels)\n",
        "\n",
        "interact(plot_model, interval=slider)"
      ],
      "metadata": {
        "id": "A5MmxQLJVHjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jKiaLcwaiNno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}